#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŸ¥çœ‹å‘é‡æ•°æ®åº“çš„å…·ä½“å†…å®¹
"""

import json
import pickle
import os
from pathlib import Path

def view_database_content():
    """æŸ¥çœ‹æ•°æ®åº“å†…å®¹"""
    print("=== åŒ»ç–—å‘é‡æ•°æ®åº“å†…å®¹æŸ¥çœ‹ ===\n")
    
    # å‘é‡å­˜å‚¨è·¯å¾„
    vector_store_path = Path("data/vector_stores")
    
    if not vector_store_path.exists():
        print(f"å‘é‡å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {vector_store_path}")
        return
    
    print(f"å‘é‡å­˜å‚¨åŸºç¡€è·¯å¾„: {vector_store_path.absolute()}")
    
    # éå†æ‰€æœ‰å‘é‡å­˜å‚¨ç›®å½•
    store_dirs = [d for d in vector_store_path.iterdir() if d.is_dir()]
    print(f"æ‰¾åˆ° {len(store_dirs)} ä¸ªå‘é‡å­˜å‚¨ç›®å½•\n")
    
    for i, store_dir in enumerate(store_dirs, 1):
        print(f"{'='*60}")
        print(f"å‘é‡å­˜å‚¨ {i}: {store_dir.name}")
        print(f"{'='*60}")
        
        # è¯»å–å…ƒæ•°æ®
        metadata_file = store_dir / "metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                print("ğŸ“‹ å…ƒæ•°æ®ä¿¡æ¯:")
                print(f"  ç§‘å®¤: {metadata.get('department', 'N/A')}")
                print(f"  æ–‡æ¡£ç±»å‹: {metadata.get('document_type', 'N/A')}")
                print(f"  ç–¾ç—…åˆ†ç±»: {metadata.get('disease_category', 'N/A')}")
                print(f"  æ–‡æ¡£æ•°é‡: {metadata.get('document_count', 'N/A')}")
                print(f"  åˆ›å»ºæ—¶é—´: {metadata.get('created_at', 'N/A')}")
                print(f"  æœ€åæ›´æ–°: {metadata.get('last_updated', 'N/A')}")
            except Exception as e:
                print(f"âŒ è¯»å–å…ƒæ•°æ®å¤±è´¥: {e}")
        else:
            print("âŒ ç¼ºå°‘metadata.jsonæ–‡ä»¶")
        
        # æ£€æŸ¥å‘é‡å­˜å‚¨æ–‡ä»¶
        faiss_file = store_dir / "index.faiss"
        pkl_file = store_dir / "index.pkl"
        
        print(f"\nğŸ“ æ–‡ä»¶çŠ¶æ€:")
        print(f"  FAISSç´¢å¼•æ–‡ä»¶: {'âœ… å­˜åœ¨' if faiss_file.exists() else 'âŒ ä¸å­˜åœ¨'}")
        print(f"  PKLæ–‡æ¡£æ–‡ä»¶: {'âœ… å­˜åœ¨' if pkl_file.exists() else 'âŒ ä¸å­˜åœ¨'}")
        
        if faiss_file.exists():
            print(f"  FAISSæ–‡ä»¶å¤§å°: {faiss_file.stat().st_size} å­—èŠ‚")
        if pkl_file.exists():
            print(f"  PKLæ–‡ä»¶å¤§å°: {pkl_file.stat().st_size} å­—èŠ‚")
        
        # å°è¯•è¯»å–æ–‡æ¡£å†…å®¹
        if pkl_file.exists():
            try:
                print(f"\nğŸ“„ æ–‡æ¡£å†…å®¹é¢„è§ˆ:")
                with open(pkl_file, 'rb') as f:
                    data = pickle.load(f)
                
                print(f"  æ•°æ®ç»“æ„ç±»å‹: {type(data)}")
                
                if isinstance(data, tuple) and len(data) >= 2:
                    docstore, index_to_docstore_id = data[0], data[1]
                    print(f"  æ–‡æ¡£å­˜å‚¨ç±»å‹: {type(docstore)}")
                    print(f"  ç´¢å¼•æ˜ å°„æ•°é‡: {len(index_to_docstore_id) if hasattr(index_to_docstore_id, '__len__') else 'N/A'}")
                    
                    # å°è¯•è·å–æ–‡æ¡£å†…å®¹
                    if hasattr(docstore, '_dict') and docstore._dict:
                        docs = list(docstore._dict.values())
                        print(f"  å®é™…æ–‡æ¡£æ•°é‡: {len(docs)}")
                        
                        # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡æ¡£çš„å†…å®¹
                        for j, doc in enumerate(docs[:3]):
                            print(f"\n  ğŸ“ æ–‡æ¡£ {j+1}:")
                            if hasattr(doc, 'page_content'):
                                content = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                                print(f"    å†…å®¹: {content}")
                            if hasattr(doc, 'metadata'):
                                print(f"    å…ƒæ•°æ®: {doc.metadata}")
                        
                        if len(docs) > 3:
                            print(f"    ... è¿˜æœ‰ {len(docs) - 3} ä¸ªæ–‡æ¡£")
                    
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
        
        print(f"\n")
    
    # æŸ¥çœ‹å¤„ç†å…ƒæ•°æ®
    print(f"{'='*60}")
    print("ğŸ“Š å¤„ç†å…ƒæ•°æ®ç»Ÿè®¡")
    print(f"{'='*60}")
    
    data_path = Path("data")
    if data_path.exists():
        processing_files = list(data_path.glob("*/processing_metadata.json"))
        print(f"æ‰¾åˆ° {len(processing_files)} ä¸ªå¤„ç†å…ƒæ•°æ®æ–‡ä»¶:")
        
        for proc_file in processing_files:
            try:
                with open(proc_file, 'r', encoding='utf-8') as f:
                    proc_data = json.load(f)
                print(f"\nğŸ“ {proc_file.parent.name}:")
                print(f"  ç§‘å®¤: {proc_data.get('department', 'N/A')}")
                print(f"  æ–‡æ¡£ç±»å‹: {proc_data.get('document_type', 'N/A')}")
                print(f"  ç–¾ç—…åˆ†ç±»: {proc_data.get('disease_category', 'N/A')}")
                print(f"  å—æ•°é‡: {proc_data.get('chunks_count', 'N/A')}")
                print(f"  å¤„ç†æ—¶é—´: {proc_data.get('processed_at', 'N/A')}")
                if 'custom_metadata' in proc_data:
                    print(f"  è‡ªå®šä¹‰å…ƒæ•°æ®: {proc_data['custom_metadata']}")
            except Exception as e:
                print(f"âŒ è¯»å– {proc_file} å¤±è´¥: {e}")

def main():
    view_database_content()

if __name__ == "__main__":
    main()